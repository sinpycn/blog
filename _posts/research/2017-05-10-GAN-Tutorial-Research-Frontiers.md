---
title: GAN介绍 - 相关研究课题
layout: post
mathjax: true
categories: [research, gan-tutorial]
---

[制作中]

GAN是一个相对新的方法， 还有很多的待解决的研究方向。

## 5.1 不收敛

GAN面对的最大的需要研究者来尝试解决的问题是不收敛问题。

很多的深度模型使用一个优化算法来寻找损失函数的小值的方式来进行训练。 
然而有很多的问题会干扰此优化过程， 优化算法通常是一个比较可靠的下降的过程。
由于，GAN需要找到针对游戏中两个玩家的平衡。
甚至如果一个玩家通过自己的更新使自己成功的下降的时候，可能会使另一个玩家向上走。
有时候两个玩家最终会达到一个平衡，但是也有其他的情景，他们反复的撤销对方的操作，而不能有实质的有用的进展。
这是一个游戏的普遍问题，而不是GAN独有的， 因此针对此问题的解决方案会有很广泛的影响。

为了直观的了解， 导数下降在游戏中（不是优化）是如何执行的， 建议读者现在去看7.2章的练习（答案在8.2章）。

同步梯度下降对有些游戏是可以收敛的，但不是全部。

在GAN的最小最大化游戏中， Goodfellow et al. (2014b)展示了同步梯度下降可以收敛，当更新在函数空间中执行时。
**在实践中， 更新实在参数空间来操作的， 因此证明以来的凸面属性并不适用。**
当前， 既没有理论的证据来说明当在DNN的参数空间更新时，GAN应该收敛，也没有理论证据证明不能收敛。

在实践中， GAN经常看起来是扰动的， 稍微有点像8.2章中模拟的例子发生的那样， 意味着他们从开始生成一种类型的样本， 到生成另一个类型的样本而完全的没有达到平衡状态。

大概GAN遇到的影响收敛的是模式塌陷问题。

### 5.1.1 模式塌陷(Mode collapse)

模式塌陷， 也被称为Helvetica scenario， 此问题发生于当生成器试图映射几个不同的$$z$$输入到同一个输出点时。
在实际应用中， 完全的模式塌陷是很少的， 但是部分的模式塌陷是很常见的。 
部分的模式塌陷场景是， 当生成器将多个图像包含一样的颜色或者同一个本质的主题， 或者多个图像包含一个狗的不同的视角。 
图22展示了模式塌陷的问题。

![Figure 22](/images/201705/10/fig22.jpg)

图22： 在一个二维模拟数据上的模式塌陷问题演示。 最上面的是模型应该学习的目标分布$$p_{data}$$。
他是一个二维空间的混合的高斯分布。 在下面的行， 我们可以看到随着GAN学习，不同时间产生的一系列的不同分布。
相对于收敛到一个分布包含所有的训练数据的模式， 生成器仅仅在一个时间点生成其中的一个模式， 在不同的模式间切换，随着判别器学习拒绝其中一个。 
此图片来自 Metz et al. (2016).

![Equation 16](/images/201705/10/eq16.jpg)

![Equation 17](/images/201705/10/eq17.jpg)

![Figure 23](/images/201705/10/fig23.jpg)

![Figure 24](/images/201705/10/fig24.jpg)

![Figure 25](/images/201705/10/fig25.jpg)

![Figure 26](/images/201705/10/fig26.jpg)

![Figure 27](/images/201705/10/fig27.jpg)

![Figure 28](/images/201705/10/fig28.jpg)

![Figure 29](/images/201705/10/fig29.jpg)

![Figure 30](/images/201705/10/fig30.jpg)

![Figure 31](/images/201705/10/fig31.jpg)

![Figure 32](/images/201705/10/fig32.jpg)


### 5.1.2

## 5.2

## 5.3

## 5.4

## 5.5

## 5.6


