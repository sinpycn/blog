---
title: GAN介绍 - 相关研究课题
layout: post
mathjax: true
categories: [research, gan-tutorial]
---

[制作中]

GAN是一个相对新的方法， 还有很多的待解决的研究方向。

## 5.1 不收敛

GAN面对的最大的需要研究者来尝试解决的问题是不收敛问题。

很多的深度模型使用一个优化算法来寻找损失函数的小值的方式来进行训练。 
然而有很多的问题会干扰此优化过程， 优化算法通常是一个比较可靠的下降的过程。
由于，GAN需要找到针对游戏中两个玩家的平衡。
甚至如果一个玩家通过自己的更新使自己成功的下降的时候，可能会使另一个玩家向上走。
有时候两个玩家最终会达到一个平衡，但是也有其他的情景，他们反复的撤销对方的操作，而不能有实质的有用的进展。
这是一个游戏的普遍问题，而不是GAN独有的， 因此针对此问题的解决方案会有很广泛的影响。

为了直观的了解， 导数下降在游戏中（不是优化）是如何执行的， 建议读者现在去看7.2章的练习（答案在8.2章）。

同步梯度下降对有些游戏是可以收敛的，但不是全部。

在GAN的最小最大化游戏中， Goodfellow et al. (2014b)展示了同步梯度下降可以收敛，当更新在函数空间中执行时。
**在实践中， 更新实在参数空间来操作的， 因此证明以来的凸面属性并不适用。**
当前， 既没有理论的证据来说明当在DNN的参数空间更新时，GAN应该收敛，也没有理论证据证明不能收敛。

在实践中， GAN经常看起来是扰动的， 稍微有点像8.2章中模拟的例子发生的那样， 意味着他们从开始生成一种类型的样本， 到生成另一个类型的样本而完全的没有达到平衡状态。

大概GAN遇到的影响收敛的是模式塌陷问题。

### 5.1.1 模式塌陷(Mode collapse)

模式塌陷， 也被称为Helvetica scenario， 此问题发生于当生成器试图映射几个不同的$$z$$输入到同一个输出点时。
在实际应用中， 完全的模式塌陷是很少的， 但是部分的模式塌陷是很常见的。 
部分的模式塌陷场景是， 当生成器将多个图像包含一样的颜色或者同一个本质的主题， 或者多个图像包含一个狗的不同的视角。 
图22展示了模式塌陷的问题。

![Figure 22](/images/201705/10/fig22.jpg)

图22： 在一个二维模拟数据上的模式塌陷问题演示。 最上面的是模型应该学习的目标分布$$p_{data}$$。
他是一个二维空间的混合的高斯分布。 在下面的行， 我们可以看到随着GAN学习，不同时间产生的一系列的不同分布。
相对于收敛到一个分布包含所有的训练数据的模式， 生成器仅仅在一个时间点生成其中的一个模式， 在不同的模式间切换，随着判别器学习拒绝其中一个。 
此图片来自 Metz et al. (2016).

模式塌陷的发生的可能是因为GAN使用最大最小（maximin）优化与使用minimax不同。 当我们寻找模型使用

![Equation 16](/images/201705/10/eq16.jpg)

$$G^{*}$$生成样本根据数据分布。 当我们将min和max的顺序颠倒以后我们发现

![Equation 17](/images/201705/10/eq17.jpg)

对应生成器的最小化位于优化过程的内循环中。 
生成器将映射每一个z到一个x上， 对应判别器判定为真而不是假的可信度。
同时同步梯度的角度看min max并没有比max min更有优势， 反之亦然。

正如3.2.6章讨论的， 模式塌陷不像是被任何特别的损失函数引起的。
**通常的主张是，模式塌陷是因为使用Jensen-Shannon散度引起的， 但是这好像不是我们这里讨论的情况， 因为GAN近似最小化$$D_{KL}(p_{data}||p_{model})$$面临同样的问题， 并且生成器也会塌陷当比Jensen-Shannon散度要求的模式少的时候。**

因为模式塌陷问题， GAN的应用经常限定到那些模型产生比较少不同的输出的问题。
只要GAN能够找到那些可以被接受的少量的输出，那么它就是有用的。
一个例子是text-to-image生成任务， 在这种任务中，输入时一个图像的描述，输出是一个符合此描述的图像。
参考图23对此种任务的描述。
一个很新的一个工作， Reed et al. (2016a)展示了针对此类任务， 其他的模型比GAN有更高的输出多样性（图24）， 但是StachGAN(Zhang et al., 2016)展示了比以前的基于GAN的方法也有更高的输出多样性（图25）.

![Figure 23](/images/201705/10/fig23.jpg)

图23： 使用GAN来做Text-to-image生成任务。 图像来自Reed et al.(2016b)

![Figure 24](/images/201705/10/fig24.jpg)

图24： 由于模式塌陷问题，对text-to-image任务，GAN有较差的输出多样性。 图片来自Reed et al. (2016a）。

![Figure 25](/images/201705/10/fig25.jpg)

图25： 比其他的基于GAN的text-to-image模型，StackGAN能够有更高的输出多样性。 图片来自Zhang et al. (2016)。

模式塌陷问题很可能是GAN最重要的问题需要研究者去解决。

其他的尝试是minibatch feature (Salimans et al., 2016)。 minibatch features基本的思路是允许判别器比较一个样本与一个minibatch的生成样本和一个minibatch的真实样本。
通过测度与这些潜在空间的其他样本距离， 判别器可以判定是否一个样本显著的与其他生成样本相似。
Minibatch features工作的很好。 
强烈建议直接复制与那些与相关介绍文章一起发布的Theano/TensorFlow代码， 因为一些小的特征定义的改动会导致很大的性能的下降。

Minibatch GAN在CIFAR-10上训练可以得到很好的结果， 多数的样本被识别为CIFAR-10的特定的类（图26）。 
当在128x128 ImageNet上训练时， 很少的样本被识别为属于ImageNet的具体的类（图27）。
图28展示了一些被精选出来的较好的图片。

![Figure 26](/images/201705/10/fig26.jpg)

图26：Minibatch GAN在CIFAR-10上训练可以得到很好的结果， 多数的样本被识别为CIFAR-10的特定的类（注释： 模型使用Label训练，也就是使用了条件式GAN）

![Figure 27](/images/201705/10/fig27.jpg)

图27： Minibatch GAN在128x128 ImageNet使用Label来训练产生图像， 有时会被识别为ImageNet的具体的类。

![Figure 28](/images/201705/10/fig28.jpg)

图28： Minibatch GAN在128x128 ImageNet上有时可能产生很好的样本， 这里的图片都是被精心挑选的。

Minibatch GAN很大程度上是为了减少模式塌陷问题而不是其他问题， 比如计数，预测的困难， 全局结构成为最显著的问题（图29， 30， 31）。
很多这些问题应该可以被解决通过设计更好的模型架构。

![Figure 29](/images/201705/10/fig29.jpg)

图29： 在128x128 ImageNet 上GAN好像有counting问题，经常产生的动物有错误数量的身体部位。

![Figure 30](/images/201705/10/fig30.jpg)

图30： 在128x128 ImageNet 上GAN在三维预测思路上有问题， 经常产生的物体太胖或者在一个轴上被对齐处理。
作为一个对读者判别网络的测试，请试着找出这里面的一张真实的图片。

![Figure 31](/images/201705/10/fig31.jpg)

图31： 在128x128 ImageNet 上GAN好像有困难去协调整体结构， 比如， 放射性奶牛，一个动物同时有四肢动物和两足动物的结构。

另一个解决模式塌陷问题的方法是unrolled GAN （Metz et al., 2016)。 理想情况下， 我们希望找到$$G^{*}=arg min_G max_D V(G,D）$$ 对两个玩家，我们会忽略掉max操作当计算G的梯度时。
实际上， 我们应该考虑$$max_D V(G,D)$$作为G的损失函数， 并且我们应该通过最大化操作来反向传播。
各种各样的通过最大化操作进行反向传递的策略存在， 但是很多，比如根据隐式差异，是不稳定的。
Unrolled GAN的思想是建立一个计算图描述$$k$$步的判别器学习， 然后当计算生成器梯度时通过所有的k步学习反向传播。 
完全的优化判别器的函数，将花费上万步， 但是Metz et al. (2016)发现展开很少的几步， 比如说10，或者更少， 也可以显著的减少模式下落问题。
这个方法还没有用到ImageNet上。 图32显示了unrolled GAN的效果在模拟数据上。

![Figure 32](/images/201705/10/fig32.jpg)

图32： UnrolledGAN可以拟合所有的混合告诉的模式在一个二维空间。 图片来自Metz et al. (2016).

### 5.1.2

## 5.2

## 5.3

## 5.4

## 5.5

## 5.6


