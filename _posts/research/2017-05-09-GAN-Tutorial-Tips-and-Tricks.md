---
title: GAN介绍 - 提示与技巧
layout: post
mathjax: true
categories: [research, gan-tutorial]
---

[制作中]

有一些个技巧可以提高GAN的表现。 比较难去说明这些技巧的有多么的有效； 有些技巧看起来对有些情况有效，但是对另一些情况起到反作用。
这些技巧只能作为值得去尝试的技术，而不是通用的最优的方法。

NIPS2016有一个对抗学习的专题讨论会， 其中Soumith Chintala将做一个关于如何训练GAN的邀请演讲。 这个演讲或多或少的与此部分的内容有相同的目标。
想了解更多的不包含在此介绍的技巧，请参考Soumith的演讲内容。 
![Soumith演讲](https://github.com/soumith/ganhacks)

### 4.1 使用Label训练

以任何形式使用标签， 模型生成的样本的质量主观上判断，无论是在形状或者构成上几乎总会有很大的提高。
Denton et al. (2015)首先观察到了这种现象， 通过使用class-conditional GAN生成的样本比较GAN生成的样本要好很多， 并且可以生成任何类的样本。
后来， Salimans et al. (2016)发现甚至当生成器不是显式的包含类信息时样本质量可以提高， 此工作说明了通过学习判别器来识别真实样本的特定的类也是很有效的提高生成样本质量的方法。

现在还不是很清楚，为什么这个技巧很有效。 可能是因为合并使用类信息给训练过程提供了有用的线索， 对优化过程有帮助。 
也可能是此方法客观上没有提高样本质量， 而只是让样本发生改变而呈现出对人类视觉友好的属性。
如果是后者， 那么这个技巧不会导致产生一个对在真实数据生成分布上的更好的模型， 但是它仍然可以帮助创建为人类观众享受的媒介， 并且可以帮助RL（增强学习）程序来做哪些依赖同样类型的与人类相关的环境以及知识的任务。

比较仅仅使用此技巧得到的结果与其他的使用同样的技巧的方法是很重要的， 比如说， 使用标签训练的模型与其他的使用标签训练的模型做比较， class-conditional模型只与其他的class-conditional模型比较。 一个使用label训练的模型与一个没有使用lable的模型做比较是不公平的，也是一个无趣的基准， 这就类似于在图像任务上，卷积网络通常会比较费卷积网络表现好。


### 4.2 单方面标签平滑

### 4.3 Virtual batch normalization(虚拟batch normalization)

### 4.4 可以平衡G和D吗？
