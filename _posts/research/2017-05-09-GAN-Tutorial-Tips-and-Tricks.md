---
title: GAN介绍 - 提示与技巧
layout: post
mathjax: true
categories: [research, gan-tutorial]
---

[制作中]

有一些个技巧可以提高GAN的表现。 比较难去说明这些技巧的有多么的有效； 有些技巧看起来对有些情况有效，但是对另一些情况起到反作用。
这些技巧只能作为值得去尝试的技术，而不是通用的最优的方法。

NIPS2016有一个对抗学习的专题讨论会， 其中Soumith Chintala将做一个关于如何训练GAN的邀请演讲。 这个演讲或多或少的与此部分的内容有相同的目标。
想了解更多的不包含在此介绍的技巧，请参考Soumith的演讲内容。 
![Soumith演讲](https://github.com/soumith/ganhacks)

### 4.1 使用Label训练

以任何形式使用标签， 模型生成的样本的质量主观上判断，无论是在形状或者构成上几乎总会有很大的提高。
Denton et al. (2015)首先观察到了这种现象， 通过使用class-conditional GAN生成的样本比较GAN生成的样本要好很多， 并且可以生成任何类的样本。
后来， Salimans et al. (2016)发现甚至当生成器不是显式的包含类信息时样本质量可以提高， 此工作说明了通过学习判别器来识别真实样本的特定的类也是很有效的提高生成样本质量的方法。

现在还不是很清楚，为什么这个技巧很有效。 可能是因为合并使用类信息给训练过程提供了有用的线索， 对优化过程有帮助。 
也可能是此方法客观上没有提高样本质量， 而只是让样本发生改变而呈现出对人类视觉友好的属性。
如果是后者， 那么这个技巧不会导致产生一个对在真实数据生成分布上的更好的模型， 但是它仍然可以帮助创建为人类观众享受的媒介， 并且可以帮助RL（增强学习）程序来做哪些依赖同样类型的与人类相关的环境以及知识的任务。

比较仅仅使用此技巧得到的结果与其他的使用同样的技巧的方法是很重要的， 比如说， 使用标签训练的模型与其他的使用标签训练的模型做比较， class-conditional模型只与其他的class-conditional模型比较。 一个使用label训练的模型与一个没有使用lable的模型做比较是不公平的，也是一个无趣的基准， 这就类似于在图像任务上，卷积网络通常会比较费卷积网络表现好。


### 4.2 单侧标签平滑

GAN试图工作通过判别器对两个密度的比率进行估计。
但是深度神经网络倾向于产生高信赖的输出来识别正确的类， 但是通常会有太极端的概率值。 
当深度网络的输入是对抗的组成， 此问题特别的突出。
分类器倾向于线性推断， 并且产生极高的信心的预测 （Goodfellow et al., 2014a）。

为了鼓励判别器来估计一个柔性的概率，而不是预测极端的信心分类， 我们使用一个被称为单侧表情平滑的技术 （Salimans et al., 2016）。

通常情况下，我们训练使用公式8， 来训练判别器。 使用TensorFlow(Abadi et al., 2015)代码来表达如下：

![Equation8 with TensorFlow](/images/201705/10/fig01.jpg)

单侧标签平滑的思想是替换目标为真实的例子 使用一个比1略小的值， 比如0.9：

![Equation8 with TensorFlow， with one-sided smoothing](/images/201705/10/fig02.jpg)

这种做法可以避免判别器的极端预测行为： 如果判别器通过学习来预测一个极端大的逻辑值，也就是对某些输入的输出概率接近于1时， 它将被惩罚并被鼓励回到一个较小的逻辑值上去。

不对伪样本的标签进行平滑处理是很重要的。 假设我们使用一个目标$$1-\alpha$$为真实数据， 并且使用目标$$0+\beta$$为伪样本。 那么最优的判别器函数为：

![Equation 15](/images/201705/10/eq15.jpg)

当$$\beta$$是零时， 那么通过$$\alpha$$的平滑将仅仅是按比例缩小判别器的最优值。
当$$\beta$$非零时， 最优判别器的函数形状会发生变化。 特别是， 在$$p_{data}(x)$$很小的且$$p_{model}(x)$$很大的区域， $$D^{*}(x)$$将会有一个山峰在一个$$p_{model}(x)$$的伪造的模式中。
判别器将鼓励生成器的不正确的行为； 从而导致生成器或者产生与数据类似的样本，或者产生与已经产生过的样本类似的样本。

单侧标签平滑是对很多以前的标签平滑技术的一个简单的修改， 标签平滑技术最早可以追溯到至少1980年代。 Szegedy et al. (2015)展示了标签平滑在识别任务的卷积网络中是一个极好的正则器。
标签平滑作为正则器如此有效的原因是它从来不鼓励模型选择非正确类别在训练集上，而是仅仅来降低正确类的信心值。
其他的正则器比如说权值衰减（Weight decay）当正则器的系数被设置的足够大，其经常鼓励一些错误分类。 Warde-Farley and Goodfellow (2016) 展示了标签平滑可以帮助降低对抗样本的**脆弱性**， 这可以理解为标签平滑应该可以帮助判别器更有效的学习如何抵抗生成器的攻击。

### 4.3 Virtual batch normalization(虚拟批量化归一化)

由于DCGAN的介绍，当前多数的GAN架构都使用了某种形式的batch normalization。 
Batch normalizaiton的主要目的是通过对模型进行重新参数化来提高模型的优化， 从而使通过一个对应此特征的简单的平均参数和一个简单的方差参数来决定每个特征的平均和方差， 而不是使用用来提取特征的有着负责结构的和相互关系的所有层的的权值。
**重新参数化通过对minibatch的数据的特征减去平均值，并除以标准方差来完成的。 归一化操作设计为模型的一部分是很重要的， 从而使反向传播计算被归一化的特征的导数。 当归一化不是被定义为模型的一部分时， 也就是说，当特征在训练后被归一化时， 这个方法将不会很有效。**



### 4.4 可以平衡G和D吗？
