---
title: GAN介绍 - 提示与技巧
layout: post
mathjax: true
categories: [research, gan-tutorial]
---

[制作中]

有一些个技巧可以提高GAN的表现。 比较难去说明这些技巧的有多么的有效； 有些技巧看起来对有些情况有效，但是对另一些情况起到反作用。
这些技巧只能作为值得去尝试的技术，而不是通用的最优的方法。

NIPS2016有一个对抗学习的专题讨论会， 其中Soumith Chintala将做一个关于如何训练GAN的邀请演讲。 这个演讲或多或少的与此部分的内容有相同的目标。
想了解更多的不包含在此介绍的技巧，请参考Soumith的演讲内容。 
![Soumith演讲](https://github.com/soumith/ganhacks)

### 4.1 使用Label训练

以任何形式使用标签， 模型生成的样本的质量主观上判断，无论是在形状或者构成上几乎总会有很大的提高。
Denton et al. (2015)首先观察到了这种现象， 通过使用class-conditional GAN生成的样本比较GAN生成的样本要好很多， 并且可以生成任何类的样本。
后来， Salimans et al. (2016)发现甚至当生成器不是显式的包含类信息时样本质量可以提高， 此工作说明了通过学习判别器来识别真实样本的特定的类也是很有效的提高生成样本质量的方法。

现在还不是很清楚，为什么这个技巧很有效。 可能是因为合并使用类信息给训练过程提供了有用的线索， 对优化过程有帮助。 
也可能是此方法客观上没有提高样本质量， 而只是让样本发生改变而呈现出对人类视觉友好的属性。
如果是后者， 那么这个技巧不会导致产生一个对在真实数据生成分布上的更好的模型， 但是它仍然可以帮助创建为人类观众享受的媒介， 并且可以帮助RL（增强学习）程序来做哪些依赖同样类型的与人类相关的环境以及知识的任务。

比较仅仅使用此技巧得到的结果与其他的使用同样的技巧的方法是很重要的， 比如说， 使用标签训练的模型与其他的使用标签训练的模型做比较， class-conditional模型只与其他的class-conditional模型比较。 一个使用label训练的模型与一个没有使用lable的模型做比较是不公平的，也是一个无趣的基准， 这就类似于在图像任务上，卷积网络通常会比较费卷积网络表现好。


### 4.2 单侧标签平滑

GAN试图工作通过判别器对两个密度的比率进行估计。
但是深度神经网络倾向于产生高信赖的输出来识别正确的类， 但是通常会有太极端的概率值。 
当深度网络的输入是对抗的组成， 此问题特别的突出。
分类器倾向于线性推断， 并且产生极高的信心的预测 （Goodfellow et al., 2014a）。

为了鼓励判别器来估计一个柔性的概率，而不是预测极端的信心分类， 我们使用一个被称为单侧表情平滑的技术 （Salimans et al., 2016）。

通常情况下，我们训练使用公式8， 来训练判别器。 使用TensorFlow(Abadi et al., 2015)代码来表达如下：

![Equation8 with TensorFlow](/images/201705/10/fig01.jpg)

单侧标签平滑的思想是替换目标为真实的例子 使用一个比1略小的值， 比如0.9：

![Equation8 with TensorFlow， with one-sided smoothing](/images/201705/10/fig02.jpg)

这种做法可以避免判别器的极端预测行为： 如果判别器通过学习来预测一个极端大的逻辑值，也就是对某些输入的输出概率接近于1时， 它将被惩罚并被鼓励回到一个较小的逻辑值上去。

不对伪样本的标签进行平滑处理是很重要的。 假设我们使用一个目标$$1-\alpha$$为真实数据， 并且使用目标$$0+\beta$$为伪样本。 那么最优的判别器函数为：

![Equation 15](/images/201705/10/eq15.jpg)

当$$\beta$$是零时， 那么通过$$\alpha$$的平滑将仅仅是按比例缩小判别器的最优值。
当$$\beta$$非零时， 最优判别器的函数形状会发生变化。 特别是， 在$$p_{data}(x)$$很小的且$$p_{model}(x)$$很大的区域， $$D^{*}(x)$$将会有一个山峰在一个$$p_{model}(x)$$的伪造的模式中。
判别器将鼓励生成器的不正确的行为； 从而导致生成器或者产生与数据类似的样本，或者产生与已经产生过的样本类似的样本。

单侧标签平滑是对很多以前的标签平滑技术的一个简单的修改， 标签平滑技术最早可以追溯到至少1980年代。 Szegedy et al. (2015)展示了标签平滑在识别任务的卷积网络中是一个极好的正则器。
标签平滑作为正则器如此有效的原因是它从来不鼓励模型选择非正确类别在训练集上，而是仅仅来降低正确类的信心值。
其他的正则器比如说权值衰减（Weight decay）当正则器的系数被设置的足够大，其经常鼓励一些错误分类。 Warde-Farley and Goodfellow (2016) 展示了标签平滑可以帮助降低对抗样本的**脆弱性**， 这可以理解为标签平滑应该可以帮助判别器更有效的学习如何抵抗生成器的攻击。

### 4.3 Virtual batch normalization(虚拟批量化标准化)

由于DCGAN的介绍，当前多数的GAN架构都使用了某种形式的batch normalization。 
Batch normalizaiton的主要目的是通过对模型进行重新参数化来提高模型的优化， 从而使通过一个对应此特征的简单的平均参数和一个简单的方差参数来决定每个特征的平均和方差， 而不是使用用来提取特征的有着负责结构的和相互关系的所有层的的权值。
**重新参数化通过对minibatch的数据的特征减去平均值，并除以标准方差来完成的。 标准化操作设计为模型的一部分是很重要的， 从而使反向传播计算被归一化的特征的导数。 当标准化不是被定义为模型的一部分时， 也就是说，当特征在训练后被标准化时， 这个方法将不会很有效。**

Batch normalization很有用， 但是对GAN来说有几个不幸的侧面的影响。 
在每一步的训练过程中，使用不同的minibatch数据来计算标准化的统计特性，会导致这些标准化常量引起波动。
当minibatch比较小的时候， 这个波动将会变得足够大， 以至于对GAN生成图像的影响比$$z$$都大。 请参考图21的例子。

![Figure 21](/images/201705/10/fig21.jpg)

图21： 两个minibatch， 每一个有16个样本， 由使用了batch noralization的生成式网络产生。
这些minibatch展示了一个问题， 偶尔会发生 当使用batch normalization时： minibatch中特征值的平均值和标准方差的波动可能有一个比独立的z对minibatch中独立的图像有很大的影响。 这里列举的是，其中一个minibatch包含全是橘黄色的彩色样本，另一个包含全是绿色的彩色样本。
minibatch中的样本应该是彼此相互独立的， 但是在这种情况下， batch normalization导致他们变得互相的有联系。

Salimans et al. (2016)介绍了一些技术来缓和此问题。 
Reference batch normalization（参考批量标准化）包含运行网络两次： 第一次是对一个minibatch的参考样本， 这里的参考样本是在训练开始以前被采样并且是保持不变的； 另一个是对当前的minibatch的样本进行训练。 
特征的平均值和标准差使用参考样本的batch进行计算。 然后，使用这些统计的信息对两个batch的特征进行标准化处理。 
此方法的一个缺点是模型容易对参考batch的样本过拟合。 
为了稍微缓解此问题， 可以使用虚拟batch normalization， **对一个样本标准化时使用的统计信息是通过一个此样本与参考batch的联合来进行计算的**。
不管是Reference batch normalization还是虚拟的batch normalizaiton都有一个特性，也就是，在训练过程的minibatch的所有样本（真实的样本，以及所有的由生成器产生的样本（除了那些定义参考batch的样本））都是被相互独立的处理， 并且是独立且同分布(i.i.d: Independent and identically distributed)。

### 4.4 可以平衡G和D吗？
