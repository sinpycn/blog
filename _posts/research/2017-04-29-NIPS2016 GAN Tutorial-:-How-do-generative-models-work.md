---
title: GAN Tutorial - How do generative models work? How do GANs compare to others?
layout: post
mathjax: true
---

[制作中.........]

现在我们了解了生成式模型的用途以及为什么它值得去使用。 现在我们可能要问， 它是如何工作的， 特别是GAN是如何工作的，以及GAN与其他生成式方法的区别？

2.1 最大似然估计

为了简便讨论， 我们先分析使用最大似然原理的生成式模型。 当然并不是所有的生成式模型都使用最大似然。 有些生成式模型默认不使用最大似然， 但是也可以做一些修改让其使用最大似然 ** （GAN就属于这一类） **。
通过忽略不使用最大似然的模型，并且分析哪些本来通常不使用最大似然的模型的最大似然版本， 可以让我们排除很多的干扰，让我们的分析变得简单。

最大似然估计是用来估计概率模型参数 $$\theta$$ 的一种方法。 最大似然方法的基本思路是定义一个模型来对概率分布进行估计。
给定一个数据集包含$$m$$个训练样本$$x^{(i)}$$， 似然（likelihood）是指模型在训练数据集上的概率， 也就是: $$\prod_{i=1}^{m}p_{model}(x^{(i)};\theta)$$。

最大似然的原理简单的说是通过优化模型参数来最大化在训练数据上的预测概率。 使用log空间最容易实现， 我们通常计算一个算术和，而不是针对每一个样本进行处理。 使用算术和使得对应模型的似然的导数的数学表达变得简单，
并且当在数字计算机上计算时， 对数值问题不敏感， 比如说，当对几个很小的概率值进行相乘计算时会出现下溢问题。

![Equation 1,2,3](/images/201704/28/eq01.jpg)

在方程式2中， 我们使用了以下特性： $$\arg \max_{v}f(v) = \arg \max_{v}\log f(v)$$ 对于正数$$v$$, 因为log函数可以对所有范围的值进行增大，但是却不影响局部的最大化。

最大似然处理过程可以参考图8.

![Figure 8](/images/201704/28/fig08.jpg)

图8： 最大似然估计过程包含， 根据数据分布来采样一些样本来构建训练数据集， 然后使用模型计算这些数据的概率，从而对此概率进行最大化处理。
这张图展示了一个针对一维数据的高斯模型， 我们可以看到不同的数据点会拉高密度函数的不同的部分。 由于这个密度函数的总和必须是1，所以不可能将所有点到达最大的概率； 随着一个点被拉高不可避免的其他的地方就会被拉低。 最终的密度函数是所有局部点的向上力的一个平衡。

我们也可以认为最大似然估计是最小化训练数据的分布（data generating distriution）与模型的KL散度：

![Equation 1,2,3](/images/201704/28/eq04.jpg)

理想情况下，如果$$p_{data}$$ 属于 $$p_{model}(x;\theta)$$ 的一种分布, 那么模型就可以很好的覆盖 $$p_{data}$$. 
在实际应用中， 我们无法获取 $$p_{data}$$ 本身， 仅仅通过包含有 $$m$$ 个样本的训练数据集。 我们通过考虑这$$m$$个样本定义了一个经验性的分布 $$\hat{p}_{data}$$ 来近似 $$p_{data}$$。 最小化$$\hat{p}_{data}$$与$$p_{data}$$的KL散度等价于在训练数据集上对log-likelihood的最大化。

关于更多的最大似然以及其他统计估计方法的内容，请参考Goodfellow et al. (2016)的第五章。

2.2 深度生成式模型的分类

如果我们只分析使用最大似然的深度生成模型， 那么我们可以比较几个模型通过比较它们计算似然和导数的方式，或者近似的计算方法。
正如我们前面提到过的，很多模型经常使用最大似然以外的原理， 为了减少比较的复杂度，我们可以评价他们的最大似然变量。 根据这种思路， 我们制作了图9的分类。
树中每一个叶子节点都有各自优点和缺点。 GAN被设计来避免很多的早已存在的缺点， 当然GAN同时也有一些新的缺点。

![Figure 9](/images/201704/28/fig09.jpg)

<span style="color:red">[需要重新确认]
图9： 此图列出了一些深度生成模型， 这些模型都是用最大似然的原理进行训练，不同之处是如何表达或者近似似然。
在分类树的左边的分支， 模型使用显式的密度函数，$$p_{model}(x;\theta)$$， 因此显式的似然被最大化。 在这些显式密度函数的模型中， 密度函数的计算有的很容易，但是也有的不容易进行计算， 所以经常需要使用变分近似，或者Monte Carlo近似（或者是两者）来进行似然函数的最大化。
右边的分支， 模型没有显式的去表达一个数据所在空间的概率分布。 
**取而代之的是， 模型通过使用一些交互的方式没有很直接的处理概率分布。通常非直接平均概率分布的交互方法，是产生样本的能力。**
有一些这种隐式的模型使用Markov chain来提供从数据分布中采样的能力； 模型定义了一种随机转换来从一个已知的样本来得到属于同一个分布的另一个样本。
其他的方法可以产生一个样本通过简单的步骤， 不需要任何的输入。 但是这种模型应用到GAN的时， 有时能被创建来定义显式的密度， GAN的训练算法使仅仅通过模型的能力来产生样本。 GAN被训练使用最右边叶子的策略， 使用一个隐式的模型通过模型直接产生符合分布的样本。</span>


2.3 显式密度模型

2.3.1 易处理的显式模型


![Equation 5](/images/201704/28/eq05.jpg)


![Figure 10](/images/201704/28/fig10.jpg)

![Equation 6](/images/201704/28/eq06.jpg)

2.3.2 显式模型需要近似

![Equation 7](/images/201704/28/eq07.jpg)

![Figure 11](/images/201704/28/fig11.jpg)

2.4 隐式密度模型

2.5 GAN与其他生成模型的比较

