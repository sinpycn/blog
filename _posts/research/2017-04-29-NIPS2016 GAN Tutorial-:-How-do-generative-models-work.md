---
title: GAN Tutorial- How do generative models work? How do GANs compare to others?
layout: post
mathjax: true
---

现在我们了解了生成式模型的用途以及为什么它值得去使用。 现在我们可能要问， 它是如何工作的， 特别是GAN如何工作的，以及GAN与其他方法的区别？

2.1 最大似然估计

为了简便讨论， 我们先分析使用最大似然原理的生成式模型。 当然并不是所有的生成式模型都使用最大似然。 **有些生成式模型默认不使用最大似然， 但是也可以去修改成使用** （GAN就属于这一类）。 
通过忽略不使用最大似然的模型，并且关注哪些原来通常不使用最大似然的模型， 我们可以排除很多不同模型间的杂乱的不同之处。

最大似然的基本思路是定义一个模型来估计概率分布， 假设使用参数 $$\theta$$ 进行参数化。 **然后，通过使用似然（likelihood）来作为概率，通过将模型应用到训练数据上** : $$\prod_{i=1}^{m}p_{model}(x^{(i)};\theta)$$, 假设一个数据集包含$$m$$个训练样本$$x^{(i)}$$。

最大似然的原理简单的说是通过优化模型参数来最大化在训练数据上的估计的概率。 这是在log空间很容易实现； 
我们通常计算一个算术和，而不是针对每一个样本进行处理。 使用算术和使得对应模型的似然的导数的数学表达变得简单。
并且当在数字计算机上计算是， 对数值问题不敏感， 比如下溢问题当对几个很小的概率值进行相乘计算时。

![Equation 1,2,3](/images/201704/28/eq01.jpg)

方程式2， 我们使用了以下特性： $$\arg \max_{v}f(v) = \arg \max_{v}\log f(v)$$ 对于正数$$v$$, 因为log函数可以对整体进行增大，但是却不影响局部的最大化。

最大似然处理过程可以参考图8.

![Figure 8](/images/201704/28/fig08.jpg)

图8： 最大似然估计过程包含， 通过数据生成分布来生产一些样本来构建训练数据集， 为了最大化在训练集上的likelihood，然后提高此模型对这些数据的预测的概率。此图说明了？？？？？？？？

我们也可以认为最大似然估计是最小化数据生成分布于模型的KL散度：

![Equation 1,2,3](/images/201704/28/eq04.jpg)

如果可以很精确的处理，那么如果$$p_{data}$$ 位于 $$p_{model}(x;\theta)$$的分布中, 那么模型可以很好的恢复 $$p_{data}$$. 
在实际应用中， 我们无法获取 $$p_{data}$$ 本身， 仅仅有包含$$m$$个样本的训练数据集。 我们使用这些样本来定义 $$\hat{p}_{data}$$, 一个经验性的分布更多的是在$$m$$个样本点上， 来近似$$p_{data}$$。 最小化$$\hat{p}_{data}$$与$$p_{data}$$的KL散度等价于对训练数据集的log-likelihood的最大化。

了解关于最大似然以及其他统计估计方法的内容，请参考Goodfellow et al. (2016)的第五章 


