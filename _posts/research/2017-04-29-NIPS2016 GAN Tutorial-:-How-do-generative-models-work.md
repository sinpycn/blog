---
title: GAN Tutorial - How do generative models work? How do GANs compare to others?
layout: post
mathjax: true
---

[制作中.........]

现在我们了解了生成式模型的用途以及为什么它值得去使用。 现在我们可能要问， 它是如何工作的， 特别是GAN是如何工作的，以及GAN与其他生成式方法的区别？

2.1 最大似然估计

为了简便讨论， 我们先分析使用最大似然原理的生成式模型。 当然并不是所有的生成式模型都使用最大似然。 有些生成式模型默认不使用最大似然， 但是也可以做一些修改让其使用最大似然 ** （GAN就属于这一类） **。
通过忽略不使用最大似然的模型，并且分析哪些本来通常不使用最大似然的模型的最大似然版本， 可以让我们排除很多的干扰，让我们的分析变得简单。

最大似然估计是用来估计概率模型参数 $$\theta$$ 的一种方法。 最大似然方法的基本思路是定义一个模型来对概率分布进行估计。
给定一个数据集包含$$m$$个训练样本$$x^{(i)}$$， 似然（likelihood）是指模型在训练数据集上的概率， 也就是: $$\prod_{i=1}^{m}p_{model}(x^{(i)};\theta)$$。

最大似然的原理简单的说是通过优化模型参数来最大化在训练数据上的预测概率。 使用log空间最容易实现， 我们通常计算一个算术和，而不是针对每一个样本进行处理。 使用算术和使得对应模型的似然的导数的数学表达变得简单，
并且当在数字计算机上计算时， 对数值问题不敏感， 比如说，当对几个很小的概率值进行相乘计算时会出现下溢问题。

![Equation 1,2,3](/images/201704/28/eq01.jpg)

在方程式2中， 我们使用了以下特性： $$\arg \max_{v}f(v) = \arg \max_{v}\log f(v)$$ 对于正数$$v$$, 因为log函数可以对所有范围的值进行增大，但是却不影响局部的最大化。

最大似然处理过程可以参考图8.

![Figure 8](/images/201704/28/fig08.jpg)

图8： 最大似然估计过程包含， 根据数据分布来采样一些样本来构建训练数据集， 然后使用模型计算这些数据的概率，从而对此概率进行最大化处理。





为了最大化在训练集上的likelihood，然后提高此模型对这些数据的预测的概率。此图说明了？？？？？？？？

我们也可以认为最大似然估计是最小化数据生成分布于模型的KL散度：

![Equation 1,2,3](/images/201704/28/eq04.jpg)

如果可以很精确的处理，那么如果$$p_{data}$$ 位于 $$p_{model}(x;\theta)$$的分布中, 那么模型可以很好的恢复 $$p_{data}$$. 
在实际应用中， 我们无法获取 $$p_{data}$$ 本身， 仅仅有包含$$m$$个样本的训练数据集。 我们使用这些样本来定义 $$\hat{p}_{data}$$, 一个经验性的分布更多的是在$$m$$个样本点上， 来近似$$p_{data}$$。 最小化$$\hat{p}_{data}$$与$$p_{data}$$的KL散度等价于对训练数据集的log-likelihood的最大化。

了解关于最大似然以及其他统计估计方法的内容，请参考Goodfellow et al. (2016)的第五章 


